---
title: "p8105_hw3_fwt2107"
author: "Felix Tran"
date: "October 4, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggridges)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "right"))
```

# Problem 1

## Data cleaning
This block of code loads and cleans the BRFSS data:

1. Load the BRFSS data from the p8105.datasets package

2. Clean the variable names

3. Only keep the observations related to the Overall Health topic

4. Remove unneeded variables

5. Transform **response** into a factor with the values ordered
from "Excellent" to "Poor"

6. Separate **locationdesc** into 2 variables (**state** and **location**) for 
the state and site for each observation

7. Remove the redundant **locationabbr** variable
```{r}
library(p8105.datasets)
brfss_df <- brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  select(-class, -topic,  -question, -c(confidence_limit_low:geo_location)) %>%
  mutate(response = forcats::fct_relevel(response, "Excellent", "Very good",
                                         "Good", "Fair", "Poor")) %>% 
  separate(locationdesc, into = c('state', 'location'), sep = ' - ') %>%
  select(-locationabbr)

brfss_df
```

## Answering questions about the data
### States observed at 7 locations in 2002
3 states were observed at 7 locations in 2002: Connecticut, Florida, and 
North Carolina.

Using the brfss_df dataset:

1. Filter observations to only look at observations from 2002

2. Group by **state**

3. Count the number of unique **location** values and only see the states with 7
location sites
```{r}
brfss_df %>% 
  filter(year == '2002') %>% 
  group_by(state) %>%
  summarize(num_sites = length(unique(location))) %>% 
  filter(num_sites == 7)
```


### Spaghetti plot
1. Group the brfss data by year and state

2. Sum up the number of observations for each state by year

3. Plot the number of observations for each state by year

4. Add labels and adjust the plot theme and legend font size
```{r}
brfss_df %>% 
  group_by(year, state) %>% 
  summarize(total_obs = sum(sample_size)) %>% 
  ggplot(aes(x = year, y = total_obs, color = state)) + 
  geom_line(alpha = 0.5) + 
  labs(title = "Number of observations in each state by year", 
       x = "Year (2002 - 2010)",
       y = "Number of observations") +
  theme(legend.text = element_text(size = 5),
        plot.title = element_text(size = 13))
```

Most states had less than 5000 observations per year. Only a few states had more
than 10,000 observations for at least 1 year of data collection.


### Table describing "Excellent" responses in NY (2002, 2006, 2010)
1. Subset the brfss data to only look at "excellent" responses for NY

2. Group observations by year

3. Calculate the mean and standard deviation for proportion of "excellent" 
responses in NY by year

4. Keep the means and standard deviations for the years 2002, 2006, 2010

5. Display results in table
```{r}
brfss_df %>% 
  filter(state == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean_excellent = mean(data_value, na.rm = T), 
            sd_excellent = sd(data_value, na.rm = T)) %>% 
  filter(year == "2002" | year == "2006" | year == "2010") %>% 
  knitr::kable(col.names = c("Year", "Mean", "Standard deviation"), 
               digits = 1,
               format = 'html',
               caption = "Proportion of 'excellent' responses in NY by year")
```

The proportion of "Excellent" responses has decreased slightly from 2002 to
2010. The standard deviation has decreased from 2002 to 2010. 

### 5 panel plot of responses over time
1. Spread the values of the **response** variable so that each answer choice in
**response** is a separate column

2. Clean the variable names and group by state and year

3. Summarize the data by calculating the average proportion of each answer 
choice by state and year

4. Gather the average proportions of each answer choice so that variable 
**response** contains the answer choices and **avg_prop** contains the average
proportion of each answer choice by state and year

5. Relevel the **response** variable so that the answers are shown in order of
"best to worst" rather than in alphabetical order

6. Plot the average proportion of each answer choice across time by state
```{r}
brfss_df %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>%
  group_by(state, year) %>%
  summarize(excellent_mean = mean(excellent, na.rm = T),
            verygood_mean = mean(very_good, na.rm = T),
            good_mean = mean(good, na.rm = T),
            fair_mean = mean(fair, na.rm = T),
            poor_mean = mean(poor, na.rm = T)) %>%
  gather(key = response, value = avg_prop, excellent_mean:poor_mean) %>%
  separate(response, into = c("response", "unneeded_var"), sep = "_") %>%
  select(-unneeded_var) %>%
  mutate(response = forcats::fct_relevel(response, "excellent", "verygood",
                                         "good", "fair", "poor")) %>%
  ggplot(aes(x = year, y = avg_prop, color = state)) +
  geom_line(alpha = 0.75) +
  labs(title = "Responses across states by year (2006 - 2010)",
       x = "Year",
       y = "Proportion of responses (%)") +
  scale_x_continuous(breaks = c(2002, 2006, 2010),
                     labels = c("2002", "2006", "2010")) +
  facet_grid(. ~ response, 
             labeller = labeller(response = c(excellent = "Excellent",
                                             verygood = "Very good",
                                             good = "Good",
                                             fair = "Fair",
                                             poor = "Poor"))) +
  theme(legend.position = "none",
        axis.text = element_text(size = 5),
        panel.spacing.x = unit(0.5, "lines"))
```

On average, the proportion of "Excellent" responses has decreased slightly 
over time across states. The proportion of "Very good" and "Good" responses
had substantial variation over time across states. The proportion of "Fair" 
responses had less variation over time. The proportion of "Poor" responses 
remained fairly consistent over time across states at close to 5% of all
responses. 

# Problem 2
## Describing the data
```{r echo = F}
cart_df <- instacart

order_history_df <- cart_df %>% 
  group_by(user_id) %>% 
  summarize(total_orders = max(order_number)) 
avg_total_orders <- round(mean(order_history_df$total_orders), digits = 0)

order_size_df <- cart_df %>% 
  group_by(order_id) %>% 
  summarize(num_items = max(add_to_cart_order))
avg_order_size <- round(mean(order_size_df$num_items), digits = 0)

```
* The dataset contains `r nrow(cart_df)` observations and 
`r ncol(cart_df)` variables. Each observation is 1 food item bought in an order.

* The dataset contains `r length(unique(cart_df$user_id))` unique users with 
`r length(unique(cart_df$order_id))` total orders. 

* Users in this dataset have placed `r avg_total_orders` orders on average 
through Instacart thus far, waiting about 
`r round(mean(cart_df$days_since_prior_order, na.rm = T), digits = 0)` days 
before placing another order.

* On average, `r avg_order_size` items are purchased per order.

* Users in this dataset have ordered items from 
`r length(unique(cart_df$department))` different departments.

```{r}
cart_df %>% 
  group_by(order_dow) %>% 
  summarize(total_orders = n()) %>%
  ggplot(aes(x = order_dow, y = total_orders)) +
  geom_bar(stat = "identity") +
  labs(title = "# of orders by day (0 - 6)")

cart_df %>% 
  group_by(order_hour_of_day) %>% 
  summarize(total_orders = n()) %>%
  ggplot(aes(x = order_hour_of_day, y = total_orders)) +
  geom_bar(stat = "identity") +
  labs(title = "# of orders by hour of day (0 - 23)")
```
Day 0 has more orders than any other day. Most orders are placed between
hours 10 - 16, i.e. 10am-4pm.



## Answering questions
### Aisles
There are `r length(unique(cart_df$aisle))` aisles, and the most items are 
ordered from fresh vegetables, fresh fruit, packaged fruits and vegetables,
yogurt, and packaged cheese.
```{r}
cart_df %>% 
  group_by(aisle) %>% 
  summarize(total_items = n()) %>% 
  arrange(desc(total_items))
```

### Graph of items ordered per aisle

1. Group items by department and aisle

2. Summarize to add up number of items ordered per aisle

3. Plot bar graph of number of ordered times by aisle. Bars are colored 
according to department to make plot more readable
```{r}
cart_df %>% 
  group_by(department, aisle) %>% 
  summarize(total_items = n()) %>% 
  ggplot(aes(x = aisle, y = total_items)) +
  geom_bar(stat = "identity", aes(fill = department)) +
  labs(title = "Number of items ordered by aisle",
       x = "Aisle",
       y = "Number of items") +
  theme(axis.text.x = element_text(size = 3, angle = 90),
        axis.line.x = element_line(size = 0)) +
  theme(legend.position = "bottom",
        legend.key.size = unit(0.75, "lines"),
        legend.text = element_text(size = 5))
```


### Table
To make the table:

1. Filter out items from different aisles

2. Group by product and aisle, then count up number of times each product was
ordered

3. Within each aisle, rank products by number of times purchased

4. Look at the most purchased item in each aisle, then display as a table
```{r}
cart_df %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | 
           aisle == "packaged vegetables fruits") %>% 
  group_by(product_name, aisle) %>% 
  summarize(num_bought = n()) %>% 
  group_by(aisle) %>% 
  mutate(num_rank = min_rank(desc(num_bought))) %>% 
  filter(num_rank == 1) %>% 
  select(aisle, product_name, -num_bought, -num_rank) %>% 
  knitr::kable(col.names = c("Aisle", "Product"),
               format = "html",
               caption = "Most popular items of selected aisles")
  
```


### Pink Lady Apples and Coffee Ice Cream
To make the table:

1. Only keep observations for pink lady apples and coffee ice cream

2. Group by product and day of the week, then calculate the mean hour at which
each item is purchased for every day

3. Spread the data so that each day is a column

4. Format data as a table
```{r}
cart_df %>% 
  filter(product_name == "Pink Lady Apples" | 
           product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_time = round(mean(order_hour_of_day), digits = 0)) %>% 
  spread(key = order_dow, value = mean_time) %>% 
  knitr::kable(col.names = c("Product", "Day 0", "Day 1", "Day 2",
                             "Day 3", "Day 4", "Day 5", "Day 6"),
               format = "html",
               caption = "Average hour of purchase")
```

With the exception of Day 5, pink lady apples on average are bought slightly 
earlier in the day than coffee ice cream.



# Problem 3
```{r echo = F}
noaa_df <- ny_noaa

noaa_df
noaa_summary <- noaa_df %>%
  mutate(tmax = as.integer(tmax), tmin = as.integer(tmin)) %>%
  summarize(mean_prcp = round(mean(prcp, na.rm = T), digits = 1),
            mean_snow = round(mean(snow, na.rm = T), digits = 1),
            mean_snwd = round(mean(snwd, na.rm = T), digits = 1),
            mean_tmin = round(mean(tmin, na.rm = T), digits = 1),
            mean_tmax = round(mean(tmax, na.rm = T), digits = 1))
prcp_na <- round((length(which(is.na(noaa_df$prcp))) / nrow(noaa_df) * 100), 
                 digits = 1)
snow_na <- round((length(which(is.na(noaa_df$snow))) / nrow(noaa_df) * 100), 
                 digits = 1)
snwd_na <- round((length(which(is.na(noaa_df$snwd))) / nrow(noaa_df) * 100), 
                 digits = 1)
tmin_na <- round((length(which(is.na(noaa_df$tmin))) / nrow(noaa_df) * 100), 
                 digits = 1)
tmax_na <- round((length(which(is.na(noaa_df$tmax))) / nrow(noaa_df) * 100), 
                 digits = 1)
prcp_na
snow_na
snwd_na
tmin_na
tmax_na
summary(noaa_df$date)
```
The dataset contains `r nrow(noaa_df)` observations and `r ncol(noaa_df)` 
variables. The data tracks precipitation, snow, snowdepth, max temperature, and
lowest temperature in NY on observed days from January 1, 1981, to December 31,
2012. 

Missing data is a big issue with this data. `r prcp_na`% of recorded days had
missing precipitation data, `r snow_na`% had missing snow data, `r snwd_na`% had
missing snow depth data, `r tmin_na`% had missing lowest temperature data, and
`r tmax_na`% had missing max temperature data.

Among the days for which data was available, average precipiation was 
`r noaa_summary$mean_prcp/10` mm, average snowfall was 
`r noaa_summary$mean_snow` mm, average snowdepth was `r noaa_summary$mean_snwd` 
mm, average lowest temperature was `r noaa_summary$mean_tmin/10` degrees 
Celsius, and average max temperature `r noaa_summary$mean_tmax/10` 
degrees Celsius.

## Answering questions
### Data cleaning and snowfall

To clean the data:

1. Transform the **date** variable into 3 variables for the year, month, and
day.

2. Convert the units for temperature from tenths of degrees Celsius to degrees
Celsius and precipitation and snowfall from tenths of mm to mm.


To obtain the most frequent observations for snow:

1. Group by snowfall observations

2. Count the frequency of each observation

3. Rank the frequencies and look at the most frequent observations

The most frequent values for snowfall are NA and 0. This makes sense because 
there is no snowfall for the majority of the year. Snowfall for most days
will either be recorded as 0 or not recorded at all.
```{r}
noaa_df <- noaa_df %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmin = as.integer(tmin) / 10, 
         tmax = as.integer(tmax) / 10,
         prcp = prcp / 10,
         snow = snow / 10,
         year = as.integer(year),
         month = as.integer(month),
         day = as.integer(day))

noaa_df %>% 
  group_by(snow) %>% 
  summarize(frequency = n()) %>% 
  mutate(freq_rank = min_rank(desc(frequency))) %>% 
  filter(freq_rank <= 5) %>% 
  arrange(freq_rank)
```

### 2-panel plot 
To make the plot:

1. Filter out observations outside of January or July

2. Group by station ID, year, and month

3. Calculate average temperature by taking the average of the lowest and highest
temperature recorded at each station

4. Calculate the average temperature across stations for each month by year

5. Plot results across time by month
```{r}
noaa_df %>% 
  filter(month == 1 | month == 7) %>% 
  group_by(id, year, month) %>% 
  mutate(tmean_daily = (tmax + tmin)/2) %>% 
  summarize(tmean_monthly = round(mean(tmean_daily, na.rm = T), digits = 1)) %>% 
  ggplot(aes(x = year, y = tmean_monthly)) +
  geom_point(size = 0.1) +
  scale_x_continuous(breaks = c(1981, 1989, 1997, 2005, 2013),
                     labels = c('1981', '1989', '1997', 
                                '2005', '2013')) +
  facet_grid(. ~ month) +
  theme(legend.position = 'none',
        panel.spacing.x = unit(0.5, "lines"),
        axis.text.x = element_text(size = 8))

```

As expected, average temperatures are lower in January compared to July. There
does not appear to be a discrenable pattern among these 2 months. There are a 
couple outliers in January where the average temperature was especially cold 
compared to preceding and subsequent years.


### Lowest and highest temperatures and snowfall plots
```{r}
library(patchwork)
temp_graph <- noaa_df %>% 
  gather(key = temp_type, value = temp_c, c(tmax, tmin)) %>% 
  ggplot(aes(x = temp_type, y = temp_c)) +
  labs(title = 'Distribution of max and min temperatures',
       x = element_blank()) +
  geom_boxplot()

noaa_snow_df <- noaa_df %>% 
  filter(snow > 0 & snow < 100) %>% 
  mutate(year = as.character(year))

snow_graph <- noaa_snow_df %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_violin() +
  labs(title = "Distribution of snowfall by year") +
  theme(axis.text.x = element_text(size = 6, angle = 90))

temp_graph / snow_graph
```

Looking at the top graph, lowest recorded temperatures appear to have more
outliers than max recorded temperatures. Snowfall distributions by year seem
to be relatively consistent.
