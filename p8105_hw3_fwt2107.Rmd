---
title: "p8105_hw3_fwt2107"
author: "Felix Tran"
date: "October 4, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggridges)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "right"))
```

# Problem 1

## Data cleaning
This block of code loads and cleans the BRFSS data:

1. Load the BRFSS data from the p8105.datasets package

2. Clean the variable names

3. Only keep the observations related to the Overall Health topic

4. Remove unneeded variables

5. Transform **response** into a factor with the values ordered
from "Excellent" to "Poor"

6. Separate **locationdesc** into 2 variables (**state** and **location**) for 
the state and site for each observation

7. Remove the redundant **locationabbr** variable
```{r}
library(p8105.datasets)
brfss_df <- brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  select(-class, -topic,  -question, -c(confidence_limit_low:geo_location)) %>%
  mutate(response = forcats::fct_relevel(response, "Excellent", "Very good",
                                         "Good", "Fair", "Poor")) %>% 
  separate(locationdesc, into = c('state', 'location'), sep = ' - ') %>%
  select(-locationabbr)

brfss_df
```

## Answering questions about the data
### States observed at 7 locations in 2002
3 states were observed at 7 locations in 2002: Connecticut, Florida, and 
North Carolina.

Using the brfss_df dataset:

1. Filter observations to only look at observations from 2002

2. Group by **state**

3. Count the number of unique **location** values and only see the states with 7
location sites
```{r}
brfss_df %>% 
  filter(year == '2002') %>% 
  group_by(state) %>%
  summarize(num_sites = length(unique(location))) %>% 
  filter(num_sites == 7)
```


### Spaghetti plot
1. Group the brfss data by year and state

2. Sum up the number of observations for each state by year

3. Plot the number of observations for each state by year

4. Add labels and adjust the plot theme and legend font size
```{r}
brfss_df %>% 
  group_by(year, state) %>% 
  summarize(total_obs = sum(sample_size)) %>% 
  ggplot(aes(x = year, y = total_obs, color = state)) + 
  geom_line(alpha = 0.5) + 
  labs(title = "Number of observations in each state by year", 
       x = "Year (2002 - 2010)",
       y = "Number of observations") +
  theme(legend.text = element_text(size = 5),
        plot.title = element_text(size = 13))
```

Most states had less than 5000 observations per year. Only a few states had more
than 10,000 observations for at least 1 year of data collection.


### Table describing "Excellent" responses in NY (2002, 2006, 2010)
1. Subset the brfss data to only look at "excellent" responses for NY

2. Group observations by year

3. Calculate the mean and standard deviation for proportion of "excellent" 
responses in NY by year

4. Keep the means and standard deviations for the years 2002, 2006, 2010

5. Display results in table
```{r}
brfss_df %>% 
  filter(state == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean_excellent = mean(data_value, na.rm = T), 
            sd_excellent = sd(data_value, na.rm = T)) %>% 
  filter(year == "2002" | year == "2006" | year == "2010") %>% 
  knitr::kable(col.names = c("Year", "Mean", "Standard deviation"), 
               digits = 1,
               format = 'html',
               caption = "Proportion of 'excellent' responses in NY by year")
```

The proportion of "Excellent" responses has decreased slightly from 2002 to
2010. The standard deviation has decreased from 2002 to 2010. 

### 5 panel plot of responses over time
1. Spread the values of the **response** variable so that each answer choice in
**response** is a separate column

2. Clean the variable names and group by state and year

3. Summarize the data by calculating the average proportion of each answer 
choice by state and year

4. Gather the average proportions of each answer choice so that variable 
**response** contains the answer choices and **avg_prop** contains the average
proportion of each answer choice by state and year

5. Relevel the **response** variable so that the answers are shown in order of
"best to worst" rather than in alphabetical order

6. Plot the average proportion of each answer choice across time by state
```{r}
brfss_df %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>%
  group_by(state, year) %>%
  summarize(excellent_mean = mean(excellent, na.rm = T),
            verygood_mean = mean(very_good, na.rm = T),
            good_mean = mean(good, na.rm = T),
            fair_mean = mean(fair, na.rm = T),
            poor_mean = mean(poor, na.rm = T)) %>%
  gather(key = response, value = avg_prop, excellent_mean:poor_mean) %>%
  separate(response, into = c("response", "unneeded_var"), sep = "_") %>%
  select(-unneeded_var) %>%
  mutate(response = forcats::fct_relevel(response, "excellent", "verygood",
                                         "good", "fair", "poor")) %>%
  ggplot(aes(x = year, y = avg_prop, color = state)) +
  geom_line(alpha = 0.75) +
  labs(title = "Responses across states by year (2006 - 2010)",
       x = "Year",
       y = "Proportion of responses (%)") +
  scale_x_continuous(breaks = c(2002, 2006, 2010),
                     labels = c("2002", "2006","2010")) +
  facet_grid(. ~ response, 
             labeller = labeller(response = c(excellent = "Excellent",
                                             verygood = "Very good",
                                             good = "Good",
                                             fair = "Fair",
                                             poor = "Poor"))) +
  theme(legend.position = "none",
        axis.text = element_text(size = 5),
        panel.spacing.x = unit(0.5, "lines"))
```

On average, the proportion of "Excellent" responses has decreased slightly 
over time across states. The proportion of "Very good" and "Good" responses
had substantial variation over time across states. The proportion of "Fair" 
responses had less variation over time. The proportion of "Poor" responses 
remained fairly consistent over time across states at close to 5% of all
responses. 

# Problem 2
## Describing the data
```{r echo = F}
cart_df <- instacart

order_history_df <- cart_df %>% 
  group_by(user_id) %>% 
  summarize(total_orders = max(order_number)) 
avg_total_orders <- round(mean(order_history_df$total_orders), digits = 0)

order_size_df <- cart_df %>% 
  group_by(order_id) %>% 
  summarize(num_items = max(add_to_cart_order))
avg_order_size <- round(mean(order_size_df$num_items), digits = 0)

```
* The dataset contains `r nrow(cart_df)` observations and 
`r ncol(cart_df)` variables. Each observation is 1 food item bought in an order.

* The dataset contains `r length(unique(cart_df$user_id))` unique users with 
`r length(unique(cart_df$order_id))` total orders. 

* Users in this dataset have placed `r avg_total_orders` orders on average 
through Instacart thus far, waiting about 
`r round(mean(cart_df$days_since_prior_order, na.rm = T), digits = 0)` days 
before placing another order.

* On average, `r avg_order_size` items are purchased per order.

* Users in this dataset have ordered items from 
`r length(unique(cart_df$department))` different departments.

## Answering questions
### Aisles
There are `r length(unique(cart_df$aisle))` aisles, and the most items are 
ordered from fresh vegetables, fresh fruit, packaged fruits and vegetables,
yogurt, and packaged cheese.
```{r}
cart_df %>% 
  group_by(aisle) %>% 
  summarize(total_items = n()) %>% 
  arrange(desc(total_items))
```

### Items ordered per aisle

