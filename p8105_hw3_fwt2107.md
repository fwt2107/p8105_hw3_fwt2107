p8105\_hw3\_fwt2107
================
Felix Tran
October 4, 2018

Problem 1
=========

Data cleaning
-------------

This block of code loads and cleans the BRFSS data:

1.  Load the BRFSS data from the p8105.datasets package

2.  Clean the variable names

3.  Only keep the observations related to the Overall Health topic

4.  Remove unneeded variables

5.  Transform **response** into a factor with the values ordered from "Excellent" to "Poor"

6.  Separate **locationdesc** into 2 variables (**state** and **location**) for the state and site for each observation

7.  Remove the redundant **locationabbr** variable

``` r
library(p8105.datasets)
brfss_df <- brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  select(-class, -topic,  -question, -c(confidence_limit_low:geo_location)) %>%
  mutate(response = forcats::fct_relevel(response, "Excellent", "Very good",
                                         "Good", "Fair", "Poor")) %>% 
  separate(locationdesc, into = c('state', 'location'), sep = ' - ') %>%
  select(-locationabbr)

brfss_df
```

    ## # A tibble: 10,625 x 6
    ##     year state location         response  sample_size data_value
    ##    <int> <chr> <chr>            <fct>           <int>      <dbl>
    ##  1  2010 AL    Jefferson County Excellent          94       18.9
    ##  2  2010 AL    Jefferson County Very good         148       30  
    ##  3  2010 AL    Jefferson County Good              208       33.1
    ##  4  2010 AL    Jefferson County Fair              107       12.5
    ##  5  2010 AL    Jefferson County Poor               45        5.5
    ##  6  2010 AL    Mobile County    Excellent          91       15.6
    ##  7  2010 AL    Mobile County    Very good         177       31.3
    ##  8  2010 AL    Mobile County    Good              224       31.2
    ##  9  2010 AL    Mobile County    Fair              120       15.5
    ## 10  2010 AL    Mobile County    Poor               66        6.4
    ## # ... with 10,615 more rows

Answering questions about the data
----------------------------------

### States observed at 7 locations in 2002

3 states were observed at 7 locations in 2002: Connecticut, Florida, and North Carolina.

Using the brfss\_df dataset:

1.  Filter observations to only look at observations from 2002

2.  Group by **state**

3.  Count the number of unique **location** values and only see the states with 7 location sites

``` r
brfss_df %>% 
  filter(year == '2002') %>% 
  group_by(state) %>%
  summarize(num_sites = length(unique(location))) %>% 
  filter(num_sites == 7)
```

    ## # A tibble: 3 x 2
    ##   state num_sites
    ##   <chr>     <int>
    ## 1 CT            7
    ## 2 FL            7
    ## 3 NC            7

### Spaghetti plot

1.  Group the brfss data by year and state

2.  Sum up the number of observations for each state by year

3.  Plot the number of observations for each state by year

4.  Add labels and adjust the plot theme and legend font size

``` r
brfss_df %>% 
  group_by(year, state) %>% 
  summarize(total_obs = sum(sample_size)) %>% 
  ggplot(aes(x = year, y = total_obs, color = state)) + 
  geom_line(alpha = 0.5) + 
  labs(title = "Number of observations in each state by year", 
       x = "Year (2002 - 2010)",
       y = "Number of observations") +
  theme(legend.text = element_text(size = 5),
        plot.title = element_text(size = 13))
```

<img src="p8105_hw3_fwt2107_files/figure-markdown_github/unnamed-chunk-3-1.png" width="90%" />

Most states had less than 5000 observations per year. Only a few states had more than 10,000 observations for at least 1 year of data collection.

### Table describing "Excellent" responses in NY (2002, 2006, 2010)

1.  Subset the brfss data to only look at "excellent" responses for NY

2.  Group observations by year

3.  Calculate the mean and standard deviation for proportion of "excellent" responses in NY by year

4.  Keep the means and standard deviations for the years 2002, 2006, 2010

5.  Display results in table

``` r
brfss_df %>% 
  filter(state == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean_excellent = mean(data_value, na.rm = T), 
            sd_excellent = sd(data_value, na.rm = T)) %>% 
  filter(year == "2002" | year == "2006" | year == "2010") %>% 
  knitr::kable(col.names = c("Year", "Mean", "Standard deviation"), 
               digits = 1,
               format = 'html',
               caption = "Proportion of 'excellent' responses in NY by year")
```

<table>
<caption>
Proportion of 'excellent' responses in NY by year
</caption>
<thead>
<tr>
<th style="text-align:right;">
Year
</th>
<th style="text-align:right;">
Mean
</th>
<th style="text-align:right;">
Standard deviation
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
2002
</td>
<td style="text-align:right;">
24.0
</td>
<td style="text-align:right;">
4.5
</td>
</tr>
<tr>
<td style="text-align:right;">
2006
</td>
<td style="text-align:right;">
22.5
</td>
<td style="text-align:right;">
4.0
</td>
</tr>
<tr>
<td style="text-align:right;">
2010
</td>
<td style="text-align:right;">
22.7
</td>
<td style="text-align:right;">
3.6
</td>
</tr>
</tbody>
</table>
The proportion of "Excellent" responses has decreased slightly from 2002 to 2010. The standard deviation has decreased from 2002 to 2010.

### 5 panel plot of responses over time

1.  Spread the values of the **response** variable so that each answer choice in **response** is a separate column

2.  Clean the variable names and group by state and year

3.  Summarize the data by calculating the average proportion of each answer choice by state and year

4.  Gather the average proportions of each answer choice so that variable **response** contains the answer choices and **avg\_prop** contains the average proportion of each answer choice by state and year

5.  Relevel the **response** variable so that the answers are shown in order of "best to worst" rather than in alphabetical order

6.  Plot the average proportion of each answer choice across time by state

``` r
brfss_df %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>%
  group_by(state, year) %>%
  summarize(excellent_mean = mean(excellent, na.rm = T),
            verygood_mean = mean(very_good, na.rm = T),
            good_mean = mean(good, na.rm = T),
            fair_mean = mean(fair, na.rm = T),
            poor_mean = mean(poor, na.rm = T)) %>%
  gather(key = response, value = avg_prop, excellent_mean:poor_mean) %>%
  separate(response, into = c("response", "unneeded_var"), sep = "_") %>%
  select(-unneeded_var) %>%
  mutate(response = forcats::fct_relevel(response, "excellent", "verygood",
                                         "good", "fair", "poor")) %>%
  ggplot(aes(x = year, y = avg_prop, color = state)) +
  geom_line(alpha = 0.75) +
  labs(title = "Responses across states by year (2006 - 2010)",
       x = "Year",
       y = "Proportion of responses (%)") +
  scale_x_continuous(breaks = c(2002, 2006, 2010),
                     labels = c("2002", "2006", "2010")) +
  facet_grid(. ~ response, 
             labeller = labeller(response = c(excellent = "Excellent",
                                             verygood = "Very good",
                                             good = "Good",
                                             fair = "Fair",
                                             poor = "Poor"))) +
  theme(legend.position = "none",
        axis.text = element_text(size = 5),
        panel.spacing.x = unit(0.5, "lines"))
```

<img src="p8105_hw3_fwt2107_files/figure-markdown_github/unnamed-chunk-5-1.png" width="90%" />

On average, the proportion of "Excellent" responses has decreased slightly over time across states. The proportion of "Very good" and "Good" responses had substantial variation over time across states. The proportion of "Fair" responses had less variation over time. The proportion of "Poor" responses remained fairly consistent over time across states at close to 5% of all responses.

Problem 2
=========

Describing the data
-------------------

-   The dataset contains 1384617 observations and 15 variables. Each observation is 1 food item bought in an order.

-   The dataset contains 131209 unique users with 131209 total orders.

-   Users in this dataset have placed 17 orders on average through Instacart thus far, waiting about 17 days before placing another order.

-   On average, 11 items are purchased per order.

-   Users in this dataset have ordered items from 21 different departments.

Answering questions
-------------------

### Aisles

There are 134 aisles, and the most items are ordered from fresh vegetables, fresh fruit, packaged fruits and vegetables, yogurt, and packaged cheese.

``` r
cart_df %>% 
  group_by(aisle) %>% 
  summarize(total_items = n()) %>% 
  arrange(desc(total_items))
```

    ## # A tibble: 134 x 2
    ##    aisle                         total_items
    ##    <chr>                               <int>
    ##  1 fresh vegetables                   150609
    ##  2 fresh fruits                       150473
    ##  3 packaged vegetables fruits          78493
    ##  4 yogurt                              55240
    ##  5 packaged cheese                     41699
    ##  6 water seltzer sparkling water       36617
    ##  7 milk                                32644
    ##  8 chips pretzels                      31269
    ##  9 soy lactosefree                     26240
    ## 10 bread                               23635
    ## # ... with 124 more rows

### Graph of items ordered per aisle

1.  Group items by department and aisle

2.  Summarize to add up number of items ordered per aisle

3.  Plot bar graph of number of ordered times by aisle. Bars are colored according to department to make plot more readable

``` r
cart_df %>% 
  group_by(department, aisle) %>% 
  summarize(total_items = n()) %>% 
  ggplot(aes(x = aisle, y = total_items)) +
  geom_bar(stat = "identity", aes(fill = department)) +
  labs(title = "Number of items ordered by aisle",
       x = "Aisle",
       y = "Number of items") +
  theme(axis.text.x = element_text(size = 3, angle = 90),
        axis.line.x = element_line(size = 0)) +
  theme(legend.position = "bottom",
        legend.key.size = unit(0.75, "lines"),
        legend.text = element_text(size = 5))
```

<img src="p8105_hw3_fwt2107_files/figure-markdown_github/unnamed-chunk-8-1.png" width="90%" />

### Table

To make the table:

1.  Filter out items from different aisles

2.  Group by product and aisle, then count up number of times each product was ordered

3.  Within each aisle, rank products by number of times purchased

4.  Look at the most purchased item in each aisle, then display as a table

``` r
cart_df %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | 
           aisle == "packaged vegetables fruits") %>% 
  group_by(product_name, aisle) %>% 
  summarize(num_bought = n()) %>% 
  group_by(aisle) %>% 
  mutate(num_rank = min_rank(desc(num_bought))) %>% 
  filter(num_rank == 1) %>% 
  select(aisle, product_name, -num_bought, -num_rank) %>% 
  knitr::kable(col.names = c("Aisle", "Product"),
               format = "html",
               caption = "Most popular items of selected aisles")
```

<table>
<caption>
Most popular items of selected aisles
</caption>
<thead>
<tr>
<th style="text-align:left;">
Aisle
</th>
<th style="text-align:left;">
Product
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
baking ingredients
</td>
<td style="text-align:left;">
Light Brown Sugar
</td>
</tr>
<tr>
<td style="text-align:left;">
packaged vegetables fruits
</td>
<td style="text-align:left;">
Organic Baby Spinach
</td>
</tr>
<tr>
<td style="text-align:left;">
dog food care
</td>
<td style="text-align:left;">
Snack Sticks Chicken & Rice Recipe Dog Treats
</td>
</tr>
</tbody>
</table>
### Pink Lady Apples and Coffee Ice Cream

To make the table:

1.  Only keep observations for pink lady apples and coffee ice cream

2.  Group by product and day of the week, then calculate the mean hour at which each item is purchased for every day

3.  Spread the data so that each day is a column

4.  Format data as a table

``` r
cart_df %>% 
  filter(product_name == "Pink Lady Apples" | 
           product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_time = round(mean(order_hour_of_day), digits = 0)) %>% 
  spread(key = order_dow, value = mean_time) %>% 
  knitr::kable(col.names = c("Product", "Day 0", "Day 1", "Day 2",
                             "Day 3", "Day 4", "Day 5", "Day 6"),
               format = "html",
               caption = "Average hour of purchase")
```

<table>
<caption>
Average hour of purchase
</caption>
<thead>
<tr>
<th style="text-align:left;">
Product
</th>
<th style="text-align:right;">
Day 0
</th>
<th style="text-align:right;">
Day 1
</th>
<th style="text-align:right;">
Day 2
</th>
<th style="text-align:right;">
Day 3
</th>
<th style="text-align:right;">
Day 4
</th>
<th style="text-align:right;">
Day 5
</th>
<th style="text-align:right;">
Day 6
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Coffee Ice Cream
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
14
</td>
</tr>
<tr>
<td style="text-align:left;">
Pink Lady Apples
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
12
</td>
</tr>
</tbody>
</table>
With the exception of Day 5, pink lady apples on average are bought slightly earlier in the day than coffee ice cream.

Problem 3
=========

    ## # A tibble: 2,595,176 x 7
    ##    id          date        prcp  snow  snwd tmax  tmin 
    ##    <chr>       <date>     <int> <int> <int> <chr> <chr>
    ##  1 US1NYAB0001 2007-11-01    NA    NA    NA <NA>  <NA> 
    ##  2 US1NYAB0001 2007-11-02    NA    NA    NA <NA>  <NA> 
    ##  3 US1NYAB0001 2007-11-03    NA    NA    NA <NA>  <NA> 
    ##  4 US1NYAB0001 2007-11-04    NA    NA    NA <NA>  <NA> 
    ##  5 US1NYAB0001 2007-11-05    NA    NA    NA <NA>  <NA> 
    ##  6 US1NYAB0001 2007-11-06    NA    NA    NA <NA>  <NA> 
    ##  7 US1NYAB0001 2007-11-07    NA    NA    NA <NA>  <NA> 
    ##  8 US1NYAB0001 2007-11-08    NA    NA    NA <NA>  <NA> 
    ##  9 US1NYAB0001 2007-11-09    NA    NA    NA <NA>  <NA> 
    ## 10 US1NYAB0001 2007-11-10    NA    NA    NA <NA>  <NA> 
    ## # ... with 2,595,166 more rows

    ## [1] 5.6

    ## [1] 14.7

    ## [1] 22.8

    ## [1] 43.7

    ## [1] 43.7

    ##         Min.      1st Qu.       Median         Mean      3rd Qu. 
    ## "1981-01-01" "1988-11-29" "1997-01-21" "1997-01-01" "2005-09-01" 
    ##         Max. 
    ## "2010-12-31"

The dataset contains 2595176 observations and 7 variables. The data tracks precipitation, snow, snowdepth, max temperature, and lowest temperature in NY on observed days from January 1, 1981, to December 31, 2012.

Missing data is a big issue with this data. 5.6% of recorded days had missing precipitation data, 14.7% had missing snow data, 22.8% had missing snow depth data, 43.7% had missing lowest temperature data, and 43.7% had missing max temperature data.

Among the days for which data was available, average precipiation was 2.98 mm, average snowfall was 5 mm, average snowdepth was 37.3 mm, average lowest temperature was 3.03 degrees Celsius, and average max temperature 13.98 degrees Celsius.

Answering questions
-------------------

### Data cleaning and snowfall

To clean the data:

1.  Transform the **date** variable into 3 variables for the year, month, and day.

2.  Convert the units for temperature from tenths of degrees Celsius to degrees Celsius and precipitation and snowfall from tenths of mm to mm.

To obtain the most frequent observations for snow:

1.  Group by snowfall observations

2.  Count the frequency of each observation

3.  Rank the frequencies and look at the most frequent observations

The most frequent values for snowfall are NA and 0. This makes sense because there is no snowfall for the majority of the year. Snowfall for most days will either be recorded as 0 or not recorded at all.

``` r
noaa_df <- noaa_df %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(tmin = as.integer(tmin) / 10, 
         tmax = as.integer(tmax) / 10,
         prcp = prcp / 10,
         snow = snow / 10,
         year = as.integer(year),
         month = as.integer(month),
         day = as.integer(day))

noaa_df %>% 
  group_by(snow) %>% 
  summarize(frequency = n()) %>% 
  mutate(freq_rank = min_rank(desc(frequency))) %>% 
  filter(freq_rank <= 5) %>% 
  arrange(freq_rank)
```

    ## # A tibble: 5 x 3
    ##    snow frequency freq_rank
    ##   <dbl>     <int>     <int>
    ## 1   0     2008508         1
    ## 2  NA      381221         2
    ## 3   2.5     31022         3
    ## 4   1.3     23095         4
    ## 5   5.1     18274         5

### 2-panel plot

To make the plot:

1.  Filter out observations outside of January or July

2.  Group by station ID, year, and month

3.  Calculate average temperature by taking the average of the lowest and highest temperature recorded at each station

4.  Calculate the average temperature across stations for each month by year

5.  Plot results across time by month

``` r
noaa_df %>% 
  filter(month == 1 | month == 7) %>% 
  group_by(id, year, month) %>% 
  mutate(tmean_daily = (tmax + tmin)/2) %>% 
  summarize(tmean_monthly = round(mean(tmean_daily, na.rm = T), digits = 1)) %>% 
  ggplot(aes(x = year, y = tmean_monthly)) +
  geom_point(size = 0.1) +
  scale_x_continuous(breaks = c(1981, 1989, 1997, 2005, 2013),
                     labels = c('1981', '1989', '1997', 
                                '2005', '2013')) +
  facet_grid(. ~ month) +
  theme(legend.position = 'none',
        panel.spacing.x = unit(0.5, "lines"),
        axis.text.x = element_text(size = 8))
```

    ## Warning: Removed 5970 rows containing missing values (geom_point).

<img src="p8105_hw3_fwt2107_files/figure-markdown_github/unnamed-chunk-13-1.png" width="90%" />

As expected, average temperatures are lower in January compared to July. There does not appear to be a discrenable pattern among these 2 months. There are a couple outliers in January where the average temperature was especially cold compared to preceding and subsequent years.

### Lowest and highest temperatures and snowfall plots

``` r
library(patchwork)
temp_graph <- noaa_df %>% 
  gather(key = temp_type, value = temp_c, c(tmax, tmin)) %>% 
  ggplot(aes(x = temp_type, y = temp_c)) +
  labs(title = 'Distribution of max and min temperatures',
       x = element_blank()) +
  geom_boxplot()

noaa_snow_df <- noaa_df %>% 
  filter(snow > 0 & snow < 100) %>% 
  mutate(year = as.character(year))

snow_graph <- noaa_snow_df %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_violin() +
  labs(title = "Distribution of snowfall by year") +
  theme(axis.text.x = element_text(size = 6, angle = 90))

temp_graph / snow_graph
```

    ## Warning: Removed 2268778 rows containing non-finite values (stat_boxplot).

<img src="p8105_hw3_fwt2107_files/figure-markdown_github/unnamed-chunk-14-1.png" width="90%" />

Looking at the top graph, lowest recorded temperatures appear to have more outliers than max recorded temperatures. Snowfall distributions by year seem to be relatively consistent.
